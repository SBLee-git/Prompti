{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma DB í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\job311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ê³µê³  ì²˜ë¦¬ ì¤‘ (1/3): https://www.wanted.co.kr/wd/270338\n",
      "âœ… ì €ì¥ ì™„ë£Œ: NLP AI ì—”ì§€ë‹ˆì–´ (4ê°œ ë¬¸ì¥)\n",
      "\n",
      "ğŸ” ê³µê³  ì²˜ë¦¬ ì¤‘ (2/3): https://www.wanted.co.kr/wd/263703\n",
      "âœ… ì €ì¥ ì™„ë£Œ: AI ì‚¬ì—…ê³„íšì„œ/ì œì•ˆì„œ ì‘ì„± ë‹´ë‹¹ì (2ê°œ ë¬¸ì¥)\n",
      "\n",
      "ğŸ” ê³µê³  ì²˜ë¦¬ ì¤‘ (3/3): https://www.wanted.co.kr/wd/268265\n",
      "âœ… ì €ì¥ ì™„ë£Œ: AIì†”ë£¨ì…˜ ì˜ì—…ì§ (4ê°œ ë¬¸ì¥)\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ChromaDB ì €ì¥ ì„±ê³µ.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_and_store_chromadb(csv_file_path, chroma_path):\n",
    "    # ChromaDB ì´ˆê¸°í™”\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    collection = client.get_or_create_collection(name=\"wanted_jobs\")\n",
    "\n",
    "    # ì„ë² ë”© ëª¨ë¸ ë¡œë”©\n",
    "    embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "    # Text Splitter ì„¤ì •\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "    # Selenium í™˜ê²½ ì„¤ì •\n",
    "    CHROMEDRIVER_PATH = r\"./data/chromedriver.exe\"\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # CSV íŒŒì¼ ì½ê¸°\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    try:\n",
    "        for idx, row in df.iterrows():\n",
    "            job_url = row[\"URL\"]\n",
    "            print(f\"\\nğŸ” ê³µê³  ì²˜ë¦¬ ì¤‘ ({idx+1}/{len(df)}): {job_url}\")\n",
    "\n",
    "            driver.get(job_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "\n",
    "            # í¬ì§€ì…˜ëª… ìˆ˜ì§‘\n",
    "            try:\n",
    "                position_name = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
    "            except:\n",
    "                position_name = \"N/A\"\n",
    "\n",
    "            # ê²½ë ¥ ìˆ˜ì§‘\n",
    "            try:\n",
    "                exp_elements = driver.find_elements(By.CSS_SELECTOR, \"span.JobHeader_JobHeader__Tools__Company__Info__yT4OD\")\n",
    "                experience = exp_elements[1].text.strip() if len(exp_elements) > 1 else \"N/A\"\n",
    "            except:\n",
    "                experience = \"N/A\"\n",
    "\n",
    "            # ìƒì„¸ ì •ë³´ ë²„íŠ¼ í´ë¦­\n",
    "            try:\n",
    "                more_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[span[contains(text(),'ìƒì„¸ ì •ë³´ ë” ë³´ê¸°')]]\")))\n",
    "                driver.execute_script(\"arguments[0].click();\", more_btn)\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # ì„¹ì…˜ êµ¬ë¶„í•˜ì—¬ í¬ë¡¤ë§ (í•„ìˆ˜ ì„¹ì…˜ë§Œ)\n",
    "            job_description_list = []\n",
    "            current_section = None\n",
    "\n",
    "            try:\n",
    "                desc_container = driver.find_element(By.CSS_SELECTOR, \"article.JobDescription_JobDescription__dq8G5\")\n",
    "                elements = desc_container.find_elements(By.XPATH, \".//*\")\n",
    "\n",
    "                for el in elements:\n",
    "                    tag_name = el.tag_name.lower()\n",
    "                    text = el.text.strip()\n",
    "\n",
    "                    if tag_name == \"h2\":\n",
    "                        job_description_list.append(f\"[{text}]\")\n",
    "\n",
    "                    elif tag_name == \"h3\":\n",
    "                        if \"ì£¼ìš”ì—…ë¬´\" in text:\n",
    "                            current_section = \"MainTask\"\n",
    "                            job_description_list.append(f\"\\n{text}\")\n",
    "                        elif \"ìê²©ìš”ê±´\" in text:\n",
    "                            current_section = \"Qualification\"\n",
    "                            job_description_list.append(f\"\\n{text}\")\n",
    "                        elif \"ìš°ëŒ€ì‚¬í•­\" in text:\n",
    "                            current_section = \"Preferred\"\n",
    "                            job_description_list.append(f\"\\n{text}\")\n",
    "                        else:\n",
    "                            current_section = None  # ë¶ˆí•„ìš” ì„¹ì…˜ ì œì™¸\n",
    "\n",
    "                    elif tag_name == \"span\" and current_section:\n",
    "                        job_description_list.append(text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "                continue\n",
    "\n",
    "            # ë¶„í•  ë° ì„ë² ë”© ì²˜ë¦¬\n",
    "            full_text = \"\\n\".join(job_description_list).strip()\n",
    "            if not full_text:\n",
    "                continue\n",
    "\n",
    "            split_chunks = text_splitter.split_text(full_text)\n",
    "            embeddings = embedding_model.encode(split_chunks).tolist()\n",
    "\n",
    "            # âœ… ìµœì¢…ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ DB ì €ì¥ ë°©ì‹\n",
    "            for i, chunk in enumerate(split_chunks):\n",
    "                collection.add(\n",
    "                    ids=[f\"{idx}-{i}\"],\n",
    "                    documents=[chunk],  # âœ… ë¬¸ìì—´ í•„ìˆ˜\n",
    "                    embeddings=[embeddings[i]],\n",
    "                    metadatas=[{\n",
    "                        \"PositionName\": position_name,\n",
    "                        \"Experience\": experience\n",
    "                    }]\n",
    "                )\n",
    "\n",
    "            print(f\"âœ… ì €ì¥ ì™„ë£Œ: {position_name} ({len(split_chunks)}ê°œ ë¬¸ì¥)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ì „ë°˜ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\n ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ChromaDB ì €ì¥ ì„±ê³µ.\")\n",
    "\n",
    "# ìµœì¢… ì½”ë“œ ì‹¤í–‰\n",
    "input_csv = \"./wanted_merged(1).csv\"\n",
    "chroma_db_path = \"./data/chroma_db\"\n",
    "\n",
    "scrape_and_store_chromadb(input_csv, chroma_db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ RAG êµ¬ì§ì ë§ì¶¤í˜• ì¶”ì²œ ê²°ê³¼ ğŸ”¥\n",
      "êµ¬ì§ìë‹˜ì˜ í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ í¬ì§€ì…˜ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\n",
      "\n",
      "### 1. í¬ì§€ì…˜ëª…: ë°ì´í„° ë¶„ì„ê°€ (Data Analyst)\n",
      "\n",
      "- **ìš”êµ¬ ê²½ë ¥**: ì‹ ì…\n",
      "- **ì£¼ìš” ì—…ë¬´**:\n",
      "  - ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ\n",
      "  - ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”\n",
      "  - ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\n",
      "- **ìê²©ìš”ê±´**:\n",
      "  - Python ë° Pandas ì‚¬ìš© ê²½í—˜\n",
      "  - ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬ì— ëŒ€í•œ ì´í•´\n",
      "  - ê¸°ë³¸ì ì¸ í†µê³„ ì§€ì‹\n",
      "- **ìš°ëŒ€ì‚¬í•­**:\n",
      "  - Gitì„ í†µí•œ ë²„ì „ ê´€ë¦¬ ê²½í—˜\n",
      "  - ë¨¸ì‹ ëŸ¬ë‹ ë˜ëŠ” ë°ì´í„° ë§ˆì´ë‹ ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜\n",
      "\n",
      "**ì¶”ì²œ ì´ìœ **: êµ¬ì§ìë‹˜ì€ Pythonê³¼ Pandasë¥¼ ì‚¬ìš©í•œ ê²½í—˜ì´ ìˆìœ¼ë©°, ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ì— ëŒ€í•œ ê¸°ì´ˆì ì¸ ì´í•´ê°€ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ê°€ëŠ” ë°ì´í„° ê¸°ë°˜ì˜ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ëŠ” ì—­í• ë¡œ, êµ¬ì§ìë‹˜ì˜ ê¸°ìˆ ì  ë°°ê²½ì´ ì˜ ë§ì•„ë–¨ì–´ì§‘ë‹ˆë‹¤. ë˜í•œ, ì‹ ì… í¬ì§€ì…˜ì´ê¸° ë•Œë¬¸ì— ê²½ë ¥ì— ëŒ€í•œ ë¶€ë‹´ì´ ì ìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. í¬ì§€ì…˜ëª…: ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ (Machine Learning Engineer)\n",
      "\n",
      "- **ìš”êµ¬ ê²½ë ¥**: ì‹ ì…\n",
      "- **ì£¼ìš” ì—…ë¬´**:\n",
      "  - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ ë° ë°°í¬\n",
      "  - ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ\n",
      "  - ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™”\n",
      "- **ìê²©ìš”ê±´**:\n",
      "  - Python ë° PyTorch ì‚¬ìš© ê²½í—˜\n",
      "  - ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ê¸°ë³¸ ì§€ì‹\n",
      "  - LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜\n",
      "- **ìš°ëŒ€ì‚¬í•­**:\n",
      "  - Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê²½í—˜\n",
      "  - Gitì„ í†µí•œ í˜‘ì—… ê²½í—˜\n",
      "\n",
      "**ì¶”ì²œ ì´ìœ **: êµ¬ì§ìë‹˜ì€ LLMì„ ì´ìš©í•œ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìˆìœ¼ë©°, PyTorchì™€ Transformersì— ëŒ€í•œ ì‚¬ìš© ê²½í—˜ë„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ëŠ” ì´ëŸ¬í•œ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ìµœì í™”í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, êµ¬ì§ìë‹˜ì˜ ê²½í—˜ì´ ë§¤ìš° ì í•©í•©ë‹ˆë‹¤. ë˜í•œ, ì‹ ì… í¬ì§€ì…˜ìœ¼ë¡œ ê²½ë ¥ì— ëŒ€í•œ ë¶€ë‹´ì´ ì ì–´ ë„ì „í•˜ê¸° ì¢‹ì€ ê¸°íšŒì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë‘ ê°€ì§€ í¬ì§€ì…˜ì€ êµ¬ì§ìë‹˜ì˜ ê¸°ìˆ ì  ë°°ê²½ê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ì˜ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# API í‚¤ ì„¤ì • \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# ChromaDB ê²½ë¡œ ì„¤ì •\n",
    "chroma_db_path = \"./data/chroma_db\"\n",
    "\n",
    "# Embedding ëª¨ë¸ ì„¤ì • (ChromaDB ì €ì¥ ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ ëª¨ë¸)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large-instruct\"\n",
    ")\n",
    "\n",
    "# ChromaDBì—ì„œ ë²¡í„°DB ë¡œë“œ (LangChain ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=chroma_db_path,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Retriever ì„¤ì • (ìœ ì‚¬í•œ ë¬¸ì„œ 2ê°œ ê²€ìƒ‰)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ì‚¬ìš©ì í”„ë¡œí•„ ì…ë ¥ ì˜ˆì‹œ (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ì„ í™œìš©)\n",
    "user_profile = \"\"\"\n",
    "ì €ëŠ” ì‹ ì…ì…ë‹ˆë‹¤. LLMì„ ì´ìš©í•˜ì—¬ ê°€ì‚¬ ìƒì„±, ì¼ê¸° ìƒì„±ì˜ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìˆê³ , pandas, git, python, transformers,pytorch ë“± ì‚¬ìš© ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "ë‚˜í•œí…Œ ì•Œë§ëŠ” ì§ë¬´ê°€ ëª¨ê°€ ìˆë‚˜ìš”?\n",
    "\"\"\"\n",
    "\n",
    "# Retrieverë¥¼ í†µí•´ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retrieved_docs = retriever.get_relevant_documents(user_profile)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# RAGë¥¼ ìœ„í•œ ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì§ìì—ê²Œ ê°€ì¥ ì í•©í•œ í¬ì§€ì…˜ì„ ì¶”ì²œí•˜ì„¸ìš”.\n",
    "\n",
    "[ë‚´ìš©]:\n",
    "{context}\n",
    "\n",
    "[êµ¬ì§ì í”„ë¡œí•„]:\n",
    "{user_profile}\n",
    "\n",
    "ì¶”ì²œ ê°€ì´ë“œë¼ì¸:\n",
    "- êµ¬ì§ìì—ê²Œ ê°€ì¥ ì˜ ë§ëŠ” í¬ì§€ì…˜ì„ ìµœëŒ€ 2ê°œ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
    "- ê° í¬ì§€ì…˜ì˜ í¬ì§€ì…˜ëª…, ìš”êµ¬ ê²½ë ¥, ì£¼ìš” ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­ì„ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "- ê° ì¶”ì²œì— ëŒ€í•´ êµ¬ì§ìì˜ í”„ë¡œí•„ê³¼ ì–´ë–¤ ì ì´ ì¼ì¹˜í•˜ëŠ”ì§€ ëª…í™•íˆ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "- ì¶”ì²œì˜ ì´ìœ ë„ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "                                          \n",
    "\n",
    "ì¶”ì²œ ê²°ê³¼:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì™„ì„± í›„ LLM ì „ë‹¬\n",
    "final_prompt = prompt.format_messages(context=context, user_profile=user_profile)\n",
    "\n",
    "# LLM í˜¸ì¶œ \n",
    "response = llm.invoke(final_prompt)\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "print(\" RAG êµ¬ì§ì ë§ì¶¤í˜• ì¶”ì²œ ê²°ê³¼ \")\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
