import torch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from backend.generator.interview_retriever import get_qa_retriever

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

# ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸
skill_prompt = PromptTemplate.from_template("""
ì•„ë˜ëŠ” íŠ¹ì • ì§ë¬´ ë° ê²½ë ¥ ìˆ˜ì¤€ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
ì´ ì§ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ ê¸°ìˆ  ìš”ì†Œ(í”„ë¡œê·¸ë˜ë° ì–¸ì–´, ë¼ì´ë¸ŒëŸ¬ë¦¬, í”„ë ˆì„ì›Œí¬, ë„êµ¬ ë“±)ë¥¼
ì¤„ë°”ê¿ˆ í˜•íƒœì˜ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì§ë¬´ ì„¤ëª…:
{text}

ê²½ë ¥ ìˆ˜ì¤€: {experience}

ê¸°ìˆ  ìš”ì†Œ ëª©ë¡:
""")

def extract_skills(position: str, experience: str) -> list:
    retriever = get_qa_retriever()
    query = f"{position} {experience}"

    docs = retriever.invoke(query)
    if not docs:
        return []

    job_text = "\n\n".join(doc.page_content for doc in docs)
    prompt = skill_prompt.format(text=job_text, experience=experience)
    response = llm.invoke(prompt).content

    return [line.strip("-â€¢â— ").strip() for line in response.strip().split("\n") if line.strip()]


# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒ)
# if __name__ == "__main__":
#     test_position = "AI ì—”ì§€ë‹ˆì–´"
#     test_experience = "ì‹ ì…"
#     skills = extract_skills(test_position, test_experience)
#     print(f"\nğŸ“Œ [{test_position} / {test_experience}] ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼:")
#     for i, skill in enumerate(skills, 1):
#         print(f"{i}. {skill}")
